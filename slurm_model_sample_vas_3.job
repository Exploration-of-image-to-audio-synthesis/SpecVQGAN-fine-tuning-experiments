#!/bin/bash
#SBATCH --job-name=model_sample_fnet_s
#SBATCH -M ukko
#SBATCH -o result_eval_fnet_s.txt
#SBATCH -p gpu
#SBATCH -c 80
#SBATCH -G 8
#SBATCH --constraint v100
#SBATCH -n 1
#SBATCH --mem=128G
#SBATCH -t 2-00:00:00


# exit when any command fails
set -e

# activate conda
module purge
module load Anaconda3
module load cuDNN/8.6.0.163-CUDA-11.8.0
source /wrk-vakka/appl/easybuild/opt/Anaconda3/2023.09-0/etc/profile.d/conda.sh

module list

conda activate specvqgan-updated

# if some error happens in the initialation of parallel process then you can get the debug info.
#export NCCL_DEBUG=INFO  # comment it if you are not debugging distributed parallel setup
#export NCCL_DEBUG_SUBSYS=ALL # comment it if you are not debugging distributed parallel setup

# We want names of master and slave nodes. Make sure this node (MASTER) comes first
MASTER=`/bin/hostname -s`
if (( $SLURM_JOB_NUM_NODES > 1 )); then
    SLAVES=`scontrol show hostnames $SLURM_JOB_NODELIST | grep -v $MASTER`
fi
# Get a random unused port on this host(MASTER)
MPORT=`comm -23 <(seq 49152 65535 | sort) <(ss -Htan | awk '{print $4}' | cut -d':' -f2 | sort -u) | shuf | head -n 1`
# it is ok if slaves are not defined
HOSTLIST="$MASTER $SLAVES"

# Determine the number of GPU
NUM_GPUS="${CUDA_VISIBLE_DEVICES//[^[:digit:]]/}"
NUM_GPUS=${#NUM_GPUS}


# Path to experiment
EXPERIMENT_PATH=../SpecVQGAN/logs/2024-05-04T23-09-59_vas_transformer_fnet_l

# Select a dataset here
DATASET="VAS"
# DATASET="VGGSound"

# TOP_K_OPTIONS=( "1" "16" "64" "100" "128" "256" "512" "1024" )
TOP_K_OPTIONS=( "64" )
# VGGSOUND_SAMPLES_PER_VIDEO=10
VGGSOUND_SAMPLES_PER_VIDEO=1
# VAS_SAMPLES_PER_VIDEO=100
VAS_SAMPLES_PER_VIDEO=2

if [[ "$DATASET" == "VAS" ]]; then
    EVAL_MELCEPTION_CFG="./evaluation/configs/eval_melception_vas.yaml"
    SPEC_DIR_PATH="data/vas/features/*/melspec_10s_22050hz/"
    RGB_FEATS_DIR_PATH="../SpecVQGAN/data/vas/features/*/feature_efficientnet_v2_l_dim1280_21.5fps/"
    SAMPLES_FOLDER="VAS_validation"
    SPLITS="[train,validation]"
    SAMPLER_BATCHSIZE=64
    SAMPLES_PER_VIDEO=$VAS_SAMPLES_PER_VIDEO
else
    echo "NotImplementedError"
    exit
fi

# Some info to print
echo "Hostlist:" $HOSTLIST
echo "Samples per video:" $SAMPLES_PER_VIDEO "; Sampler path" $EXPERIMENT_PATH
echo $SPEC_DIR_PATH
echo $RGB_FEATS_DIR_PATH
echo $FLOW_FEATS_DIR_PATH

for TOP_K in "${TOP_K_OPTIONS[@]}"; do
    echo "Starting TOP-$TOP_K. Number of Samples/Video: $SAMPLES_PER_VIDEO"

    # Saving the time stamp to reuse it when sampling is done. Random second is used to avoid overalapping \
    # sample folder names
    RAND_SEC=$((RANDOM%59+1))
    NOW=`date +"%Y-%m-%dT%H-%M-$(printf %02d $RAND_SEC)"`

    # Launch the torch.distributed.launch tool, first on master (first in $HOSTLIST) then on the slaves
    # Escape the '$' from the variable if you want to take variable from the server's environement (where you ssh)
    # By default bash will execute the tring with the variables from this script
    # loading conda environment.
    # We are doing both sampling and evaluation sequentially
    NODE_RANK=0
    python -m torch.distributed.launch \
        --nproc_per_node=$NUM_GPUS \
        --nnodes=$SLURM_JOB_NUM_NODES \
        --node_rank=$NODE_RANK \
        --master_addr=$MASTER \
        --master_port=$MPORT \
        --use_env \
            evaluation/generate_samples.py \
            sampler.config_sampler=evaluation/configs/sampler.yaml \
            sampler.model_logdir=$EXPERIMENT_PATH \
            sampler.splits=$SPLITS \
            sampler.samples_per_video=$SAMPLES_PER_VIDEO \
            sampler.batch_size=$SAMPLER_BATCHSIZE \
            sampler.top_k=$TOP_K \
            data.params.spec_dir_path=$SPEC_DIR_PATH \
            data.params.rgb_feats_dir_path=$RGB_FEATS_DIR_PATH \
            sampler.now=$NOW
    python -m torch.distributed.launch \
        --nproc_per_node=$NUM_GPUS \
        --nnodes=$SLURM_JOB_NUM_NODES \
        --node_rank=$NODE_RANK \
        --master_addr=$MASTER \
        --master_port=$MPORT \
        --use_env \
        evaluate.py \
            config=$EVAL_MELCEPTION_CFG \
            input2.path_to_exp=$EXPERIMENT_PATH \
            patch.specs_dir=$SPEC_DIR_PATH \
            patch.spec_dir_path=$SPEC_DIR_PATH \
            patch.rgb_feats_dir_path=$RGB_FEATS_DIR_PATH \
            input1.params.root=$EXPERIMENT_PATH/samples_$NOW/$SAMPLES_FOLDER
echo "Done TOP-$TOP_K"
done
